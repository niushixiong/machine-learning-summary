1 语义相似度计算都有哪些方法
 1.1 语义相似的计算方法
  
  1 tfidf + 余弦相似度（其他方法）
  2 词向量 结合tfidf + 余弦相似度
  3 主题模型lsa
  4 doc2vec + 余弦相似度
  5 bm2.5（todo）
  6 simhash（todo）
  7 dssm 语义模型
    dssm（cnn-dssm lstm-dssm）
    使用DNN将 query和title的海量点击曝光日志，表示为低维度语义向量，并通过cosine计算距离，最终训练语义相似度模型。可以用来预测两个句子的语义相似度
    或者获得某个句子的低纬度语义向量表达
    三层    输入层                              表示层                     匹配层
    sparse-》embbeding   
    英文 word hashing ，letter-trigams ；    dnn 最后输出层tanh 128         softmax 计算query 与正doc的后验概率
    中文单字
    
    极大似然估计，最小化损失函数 +sgd
    dssm使用子向量减少切词依赖，提高泛化能力，每个词语义可复用，无监督的词向量并结合有监督的训练减小误差。但丧失了语序信息和上下文信息，
    cnn-dssm 在输入和表示之间 加入滑动窗口 word ngram
    lstm-dssm （todo）
    https://blog.csdn.net/Jaygle/article/details/80927732
   8 bert
    
 1.2 相似度以及距离定义 
    编辑距离 wmd 余弦距离 欧氏距离 曼哈顿距离 明科夫斯基距离 jaccard相似系数 皮尔逊相关系数 simhash + 汉明距离 斯皮尔曼（等级）相关系数 bm2.5
   https://www.cnblogs.com/shona/p/11971310.html
  
 1.3 语义相似度计算任务思路
     句子长度 词的个数  相似度系数 编辑距离 等特征
     https://zhuanlan.zhihu.com/p/51675979
 
2 文本分类
 2.1 textCNN 
  
    
 
 2.2 fast-text
   类似于Word2vec的cbow模型，其也是三层神经网络模型，只不过最后的层不再是计算中心词的概率而是输出每个类别的概率；
   输入层n-gram， 隐藏层，输出层 层次化的softmax；
         xi: 一个句子的特征，初始值为随机生成(也可以采用预训练的词向量)
         hidden: xi的平均值
         output: 样本标签
      l = -1/N Sum[yn*log(f(xn*A*B))
   fastText的核心思想就是：
       将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。
       这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。
   https://zhuanlan.zhihu.com/p/32965521?from_voters_page=true
   https://www.cnblogs.com/huangyc/p/9768872.html
   https://blog.csdn.net/ymaini/article/details/81489599
   相似处：
         图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。
         都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。
   不同处：
         模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是分类的label。不过不 
      管输出层对应的是什么内容，起对应的vector都不会被保留和使用。
         模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容。
   两者本质的不同，体现在 h-softmax的使用：
         Word2vec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的 h-softmax也会生成一系列的向量，但最终都被抛弃，不会使用。
         fastText则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）

 2.3 bert如何做文本分类
