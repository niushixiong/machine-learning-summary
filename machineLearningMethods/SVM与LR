1、LR原理与推导
  1.1 原理推导
  1.2 交叉熵损失函数
  1.3 LR为什么simod函数
    平滑可导；指数族分布与广义线性模型推导；
  1.4 LR要用交叉熵损失而不是平方损失
    如果平方损失，非凸函数； 如果用平方差在求梯度是与sigmod有关，当模型输出接近0或1时，梯度很小，收敛会很慢，出现梯度消失；
  1.5 LR为什么用离散特征
    离散特征的增加和减少都很容易，易于模型快速迭代；离散化后稀疏矩阵计算速度更快；离散化后增加了非线性；鲁棒性，更稳定；离散化后特征交叉，进一步增加非线性表达能力；
  1.6 线性回归对比LR
    一个分类一个回归；最小二乘法与交叉损失；
    1.6.1 L1与L2区别
     L1稀疏化：概率角度高斯与拉普拉斯分布；损失函数与参数解空间图像交集；函数求导
  1.7 牛顿法与梯度下降法
    牛顿法二阶泰勒展开，梯度下降一阶泰勒展开；二阶收敛对比一阶收敛
    梯度下降在接近最优值时步长需要更新不段减小，否则会出现震荡；牛顿法的二阶导数的倒数相当于梯度下降的学习率，更新中起到不断缩小步长的作用，其收敛速度相对于梯度下降更快；
    牛顿法的缺点是 二阶必须可导，海森矩阵的逆计算复杂度高，代价大
    
     
  
2、SVM原理
  2.1 原理
  2.2 非线性与核函数
  
3、LR与SVM区别
  LR损失函数交叉熵与SVM合页损失；
  LR参数模型，SVM非参数模型；
  SVM加入了正则化，结构经验风险最小化；
  SVM与支持向量机有关，LR与所有样本有关；
  对于非线性问题处理，SVM使用核函数，LR不采用核函数，采用特征组合；
  
